{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-f14c00bcf756>, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-f14c00bcf756>\"\u001b[0;36m, line \u001b[0;32m79\u001b[0m\n\u001b[0;31m    result_dic = requests.get(apiUrl).text[]\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import language\n",
    "\n",
    "\n",
    "class NewspaperScraper():\n",
    "\n",
    "    language_client = language.Client()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dic = {}  # creates a new empty dic for each instance\n",
    "        self.article_links = []  # creates a new empty list for each instance\n",
    "        self.sentiment_data = {}\n",
    "        self.entity_data = {}\n",
    "\n",
    "    def newspaper(self, url, authors='authors'):\n",
    "        article = Article(url)   \n",
    "\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            r_datetime = article.publish_date\n",
    "            d = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "            t = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "            raw_text = article.text\n",
    "            if authors =='authors':\n",
    "                author = article.authors[0]\n",
    "            else:\n",
    "                author = 'none'\n",
    "            return d, t, raw_text, author\n",
    "        except:\n",
    "            return 'none', 'none', 'none', 'none'\n",
    "\n",
    "    def googlify(text):\n",
    "        document = language_client.document_from_text(text)\n",
    "\n",
    "        sentiment = document.analyze_sentiment()\n",
    "        self.sentiment_data = {'score':sentiment.score, 'magnitude':sentiment.magnitude}\n",
    "\n",
    "        entities = document.analyze_entities()\n",
    "        for entity in entities:\n",
    "            self.entity_data[entity.name] = {'type':entity.type, \n",
    "                                        'salience':entity.salience, \n",
    "                                        'wiki':entity.wikipedia_url}\n",
    "        return self.sentiment_data, self.entity_data\n",
    "\n",
    "\n",
    "    def dictify(self, d, t, author, raw_text):\n",
    "        if d in self.dic:\n",
    "            self.dic[d].append([{'time':t,\n",
    "                                    'author':author,\n",
    "                                    'raw_text':raw_text, \n",
    "                                    'sentiment':self.sentiment_data,\n",
    "                                    'entities':self.entity_data}])\n",
    "        else:\n",
    "            self.dic[d] = [{'time':t,\n",
    "                            'author':author,\n",
    "                            'raw_text':raw_text, \n",
    "                            'sentiment':self.sentiment_data,\n",
    "                            'entities':self.entity_data}]\n",
    "        return self.dic\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Guardian(NewspaperScraper):\n",
    "\n",
    "    base_url = 'http://www.theguardian.com' # same for every instance\n",
    "    apiKey = '458f61b9-2ff0-4a3f-98a1-feff65660ca6'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.article_links = []  # creates a new empty  for each instance\n",
    "\n",
    "\n",
    "    def getlinks(self, name):\n",
    "        #  uses the api unlike others\n",
    "        split_name = '%20'.join(name.split())\n",
    "        date = '2001-01-01'\n",
    "        apiUrl = 'http://content.guardianapis.com/search?from-date={}&page-size=161&q={}&api-key={}'.format(date, split_name, apiKey)\n",
    "\n",
    "        result_dic = requests.get(apiUrl).text\n",
    "\n",
    "        for result in json.loads(result_dic.text)['response']['results']:\n",
    "\n",
    "            if result['type'] == 'article' and result['section'] != 'sport':\n",
    "                url = result['webUrl']\n",
    "                self.article_links.append(url)\n",
    "\n",
    "        return self.article_links\n",
    "\n",
    "\n",
    "    def parse(self):\n",
    "\n",
    "        if self.article_links = None:\n",
    "            raise Exception.message('No links! Run getlinks() before parse()')\n",
    "\n",
    "        for link in self.article links:\n",
    "            d, t, author, raw_text = newspaper(link)\n",
    "            self.dictify(self.dic, d, t, author, raw_text)\n",
    "\n",
    "        return self.dic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WorldCrunch(NewspaperScraper):\n",
    "\n",
    "    base_url = 'http://www.worldcrunch.com/' # same for every instance\n",
    "\n",
    "    # and search url ?\n",
    "\n",
    "    def getlinks(self, search_url):\n",
    "\n",
    "        r = requests.get(search_url)\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        block = soup.find_all(\"div\",{\"class\":\"row\"})[4]\n",
    "\n",
    "        for b in block.find_all(\"a\"):\n",
    "            if len(b['href'].split('/')) > 6 and b['href'] not in self.article_links:\n",
    "                self.article_links.append(b['href'])\n",
    "\n",
    "        return self.article_links\n",
    "\n",
    "\n",
    "    def parse(self):\n",
    "        if self.article_links = None:\n",
    "            raise Exception.message('No links! Run getlinks() before parse()')\n",
    "\n",
    "        for link in self.article links:\n",
    "            d, t, author, raw_text = newspaper(link)\n",
    "            self.dictify(self.dic, d, t, author, raw_text)\n",
    "\n",
    "        return self.dic\n",
    "\n",
    "\n",
    "class EurActiv(NewspaperScraper):\n",
    "\n",
    "    base_url = 'http://www.euractiv.com'\n",
    "\n",
    "    def getlinks(self, name):\n",
    "        split_name = '+'.join(name.split())\n",
    "        search_url = 'http://www.euractiv.com/?s={}'.format(split_name)\n",
    "        soup = BeautifulSoup(requests.get(search_url).text)\n",
    "        block = soup.find_all(\"div\", {\"class\":\"row\"})[4]\n",
    "\n",
    "        for b in block.find_all(\"a\"):\n",
    "            if len(b['href'].split('/')) > 6 and b['href'] not in self.article_links:\n",
    "                self.article_links.append(b['href'])\n",
    "\n",
    "        return self.article_links\n",
    "\n",
    "    def parse(self):\n",
    "        if self.article_links = None:\n",
    "            raise Exception.message(\"No links! Run 'getlinks()' before 'parse()'\")\n",
    "\n",
    "        for link in self.article links:\n",
    "            d, t, author, raw_text = newspaper(link)\n",
    "            self.dictify(self.dic, d, t, author, raw_text)\n",
    "\n",
    "        return self.dic\n",
    "\n",
    "\n",
    "class BBC(NewspaperScraper):\n",
    "    base_url = 'http://www.bbc.com/news'\n",
    "\n",
    "    def getlinks(self, name):\n",
    "        split_name = '+'.join(name.split())\n",
    "        for i in range(2,10):\n",
    "            search_url = 'http://www.bbc.co.uk/search?q={}&page={}'.format(split_name, i)\n",
    "\n",
    "        r = requests.get(search_url)\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        results = soup.find_all(\"section\", {\"id\":\"search-content\"})\n",
    "        links = []\n",
    "\n",
    "        for r in results[0].find_all(\"li\"):\n",
    "            if r.find(\"a\")['href'] not in article_links:\n",
    "                links.append(r.find(\"a\")['href'])\n",
    "\n",
    "        self.article_links.extend(links)\n",
    "\n",
    "        return self.article_links  \n",
    "\n",
    "#     def parse(self):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
