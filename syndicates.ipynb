{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from newspaper import Article\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members = ['mario draghi', 'vitor constancio', 'benoit coeure', 'yves mersche', 'sabine lautenschlager', 'peter praet']\n",
    "# representatives = ['luc coene', 'jens weidmann', 'patrick honohan', 'yannis stournaras', 'lui maria linde', 'ardo hansson',\n",
    "#                    'christian noyer', 'ignazio visco', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab homepages, search from there\n",
    "start:\n",
    "- project Syndicate\n",
    "- WorldCrunch\n",
    "- Eurozine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites = ['https://www.project-syndicate.org/', 'http://www.worldcrunch.com/business-finance', 'http://www.eurozine.com/']\n",
    "r = requests.get(sites[1])\n",
    "worldcrunch_soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = worldcrunch_soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links : 50\n",
      "Other links: 86\n"
     ]
    }
   ],
   "source": [
    "title = re.compile('[A-Z][\\w-]')\n",
    "\n",
    "article_links = []\n",
    "\n",
    "no_match = 0\n",
    "\n",
    "for link in links:\n",
    "    if title.match(link.text.strip()):\n",
    "        article_links.append(link)\n",
    "    else:\n",
    "        no_match += 1\n",
    "        \n",
    "print('Number of links : {}'.format(str(len(article_links))))\n",
    "print('Other links: {}'.format(str(no_match)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: have to do this multiple times to get to 29 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# stops = ['SUBSCRIBE', 'subscribe', 'Terms of Use', 'About Us', 'Privacy Policy']\n",
    "\n",
    "for link in article_links:\n",
    "    if len(link.text.split()) < 3:\n",
    "        article_links.remove(link)\n",
    "    elif '#' in str(link):\n",
    "        article_links.remove(link)\n",
    "\n",
    "print(len(article_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use newspaper to extract articles from remaining links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = 'http://www.worldcrunch.com/'\n",
    "\n",
    "def parseArticle(base_url, article_links):\n",
    "    \n",
    "    worldcrunch_dic = {}\n",
    "\n",
    "    for link in article_links[2:]:\n",
    "        url = base_url+link['href']\n",
    "        r = requests.get(url)\n",
    "        # Beautiful Soup\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        try:\n",
    "            publish_date = soup.find(\"meta\", {\"name\":\"DISPLAYDATE\"})['content']\n",
    "            r_datetime = datetime.strptime(publish_date, '%Y-%m-%d %H:%M:%S')\n",
    "            d = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "            t = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "            author = soup.find_all(\"p\", {\"class\":\"resize-auth\"})[0].find('a').text.strip()\n",
    "            raw_text = soup.find(\"div\", {\"class\":\"article-text\"}).text\n",
    "            if d in worldcrunch_dic:\n",
    "                worldcrunch_dic[d].append([author, t, raw_text])\n",
    "            else:\n",
    "                worldcrunch_dic[d] = [(author, t, raw_text)]\n",
    "        except:\n",
    "            pass\n",
    "    return worldcrunch_dic\n",
    "\n",
    "worldcrunch_dic = parseArticle(base_url, article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('worldcrunch.json', 'w') as fp:\n",
    "    json.dump(worldcrunch_dic, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(worldcrunch_dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try methods for syndicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_url = 'https://www.project-syndicate.org/search?query=mario%20draghi&language=english&sortBy=publishedon&sortOrder=desc'\n",
    "base_url = 'https://www.project-syndicate.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(search_url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "article_links = soup.find_all(\"section\", {\"class\":\"small-12 medium-12 large-7 columns search-results listing authors\"})[0].find_all('article', {\"class\":\"section economics \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NewspaperParse(url, authors='authors'):\n",
    "    article = Article(url)   \n",
    "    article.download()\n",
    "    article.parse()\n",
    "    r_datetime = article.publish_date\n",
    "    d = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "    t = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "    raw_text = article.text\n",
    "    if authors =='authors':\n",
    "        author = article.authors[0]\n",
    "    else:\n",
    "        author = 'none'\n",
    "    \n",
    "    return d, t, raw_text, author \n",
    "\n",
    "def PS_parse(base_url, article_links):\n",
    "    ps_dic = {}\n",
    "    for link in article_links:\n",
    "        ext = link.find(\"h2\").find('a')['href']\n",
    "        url = base_url + ext\n",
    "        dt, raw_text, author = NewspaperParse(url)\n",
    "        ps_dic[dt] = (author, raw_text)\n",
    "    return ps_dic\n",
    "\n",
    "# ps_dic = PS_parse(base_url, article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ps.json', 'w') as fp:\n",
    "    json.dump(ps_dic, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/news/business-38507368'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_url = 'http://www.bbc.com/news/business'\n",
    "base_url ='http://www.bbc.com'\n",
    "\n",
    "r = requests.get(search_url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "title_ext = soup.find('div',{\"class\":\"column--primary\"}).find('a', {\"class\":\"title-link\"})['href']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup.find_all('div', {\"class\":\"pigeon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    search_url = 'http://www.bbc.co.uk/search?q=mario+draghi&page={}'.format(i)\n",
    "    r = requests.get(search_url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    results = soup.find_all(\"section\", {\"id\":\"search-content\"})\n",
    "    links = []\n",
    "    for r in results[0].find_all(\"li\"):\n",
    "        if r.find(\"a\")['href'] not in article_links:\n",
    "            links.append(r.find(\"a\")['href'])\n",
    "    \n",
    "    article_links.extend(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image copyright Getty Images\\n\\nEurozone inflation has surged to its highest rate in more than three years, driven by increased prices for energy, food, alcohol and tobacco.\\n\\nThe annual inflation rate hit 1.1% last month, according to official statistics agency Eurostat, a sharp jump from November\\'s rate of 0.6%.\\n\\nThe rate is the highest since September 2013, when inflation was also 1.1%.\\n\\nThe higher-than-expected increase brings inflation closer to the European Central Bank\\'s target of just below 2%.\\n\\nECB chief Mario Draghi has said he expects inflation to reach the target by 2018 or 2019.\\n\\nLast month\\'s increase was driven mainly by a jump in energy prices, which rose by 2.5% year-on-year in December, their first increase in over a year. Energy prices were boosted by oil cartel Opec\\'s decision to cut output.\\n\\nFood, alcohol and tobacco prices rose 1.2% year-on-year, while services were also 1.2% more expensive than a year ago.\\n\\nShort-lived rise?\\n\\nThe rise will help to allay fears that the eurozone could slip into deflation, weakening economic growth.\\n\\nHowever, while the headline rate of inflation increased sharply in December, the core rate - which excludes prices of items such as energy and food which are driven by world markets - increased only slightly from 0.8% to 0.9%.\\n\\nThe small rise could mean the jump in inflation is short-lived, analysts suggested.\\n\\nHowever, a separate survey from IHS Markit indicated that the eurozone economy expanded at its fastest rate for more than five-and-a-half years in December.\\n\\nThe survey also said that output charges - what companies price their goods at - rose for the second month running and at the steepest pace since July 2011.\\n\\n\"The survey data are signalling a 0.4% expansion of GDP in the fourth quarter,\" said IHS Markit chief economist Chris Williamson.\\n\\n\"The concern is that domestic demand is likely to remain subdued over the course of 2017 as political uncertainty dominates, resulting in another year of disappointing growth across the region as a whole.\"'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art = Article(article_links[1])\n",
    "art.download()\n",
    "art.parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art.authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li data-result-number=\"1\">\n",
      "<article class=\"has_image media-text\" itemscope=\"\">\n",
      "<aside class=\"flags top\">\n",
      "<dl>\n",
      "<dt>Published Date</dt>\n",
      "<dd>\n",
      "<time class=\"display-date\" datetime=\"2016-11-28T13:21:50Z\">\n",
      "           28 Nov 2016       </time>\n",
      "</dd>\n",
      "</dl>\n",
      "</aside>\n",
      "<a href=\"http://www.bbc.co.uk/news/live/uk-politics-parliaments-38132872\" id=\"search-result-0\">\n",
      "<picture>\n",
      "<img alt=\"\" class=\"\" src=\"http://ichef.bbci.co.uk/news/200/cpsprodpb/0347/production/_92693800_draghiap.jpg\" srcset=\"http://ichef.bbci.co.uk/news/200/cpsprodpb/0347/production/_92693800_draghiap.jpg\"/> </picture>\n",
      "</a>\n",
      "<aside class=\"flags mid\">\n",
      "<dl>\n",
      "<dt>Published Date</dt>\n",
      "<dd>\n",
      "<time class=\"display-date\" datetime=\"2016-11-28T13:21:50Z\">\n",
      "           28 Nov 2016       </time>\n",
      "</dd>\n",
      "</dl>\n",
      "</aside>\n",
      "<div>\n",
      "<h1 itemprop=\"headline\"><a href=\"http://www.bbc.co.uk/news/live/uk-politics-parliaments-38132872\">MEPs question Mario Draghi</a></h1>\n",
      "<p class=\"summary short\">…European Central Bank (ECB) chief <em>Mario</em> <em>Draghi</em> appeared before Economic Affairs…</p>\n",
      "<p class=\"summary medium\">…European Central Bank (ECB) chief <em>Mario</em> <em>Draghi</em> appeared before Economic Affairs…</p>\n",
      "<p class=\"summary long\">…European Central Bank (ECB) chief <em>Mario</em> <em>Draghi</em> appeared before Economic Affairs…</p>\n",
      "<footer>\n",
      "<dl class=\"flags btm\">\n",
      "<dt>Published Date</dt>\n",
      "<dd>\n",
      "<time class=\"display-date\" datetime=\"2016-11-28T13:21:50Z\">\n",
      "           28 Nov 2016       </time>\n",
      "</dd>\n",
      "</dl>\n",
      "<dl>\n",
      "<dt>Tags</dt>\n",
      "<dd><span class=\"signpost-site\" data-site=\"news\">News</span><span class=\"signpost-section\">Parliaments</span></dd>\n",
      "</dl>\n",
      "</footer>\n",
      "</div>\n",
      "<a class=\"rs_touch\" href=\"http://www.bbc.co.uk/news/live/uk-politics-parliaments-38132872\"></a>\n",
      "<span class=\"divide\"></span>\n",
      "</article>\n",
      "</li>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "search_content = soup.find(\"section\", {\"class\":\"search-content\"})\n",
    "for ol in search_content.find_all(\"ol\"):\n",
    "    print(ol.find(\"li\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaurdian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('guardianresults.json') as data_file:    \n",
    "    guardian_results = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guardian_dic = {}\n",
    "\n",
    "for result in guardian_results['response']['results']:\n",
    "    if result['type'] == 'article':\n",
    "        url = result['webUrl']\n",
    "        \n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        raw_text = article.text\n",
    "        author = article.authors\n",
    "        r_datetime = article.publish_date\n",
    "        d = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "        t = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "        if d in guardian_dic:\n",
    "            guardian_dic[d].extend([(author, t, raw_text)])\n",
    "        else:\n",
    "            guardian_dic[d] = [(author, t, raw_text)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Steve Boxer'],\n",
       "  '00:00:00',\n",
       "  'We can assume that David Cameron is a bit of a gamer, having been busted obsessively playing Fruit Ninja and Angry Birds in the past. So there is an outside chance that, finally, he might soften his government’s harsh, grudging response to the Syrian refugee crisis. Because one of the gaming world’s most famous characters – the moustachioed plumber Mario – has been co-opted into highlighting the plight of those attempting to flee from the Scylla and Charybdis of Isis and Bashar al-Assad.\\n\\nA Syrian artist, pseudonymously known as Samir al-Mutfi, has created Syrian Super Mario, a satirical video that is currently going viral, highlighting the horrors faced by Syrian refugees – by tweaking the 1985 video game Super Mario Bros. Mario has taken many forms over the decades, but this time he has become Refugee Mario. And instead of making his way to Bowser’s castle in order to rescue Princess Peach, he merely has to get to the castle, which now bears the forbidding legend: “Camp”.\\n\\nAlong the way, many perils (all rendered in the familiar Super Mario Bros style, with sound effects to match) await. Grabbing his suitcase and emptying his savings account by bashing a money-block with his head, Refugee Mario hands his funds to traffickers, undergoes a perilous Mediterranean crossing (thank goodness he has multiple lives), evades border-patrolling soldiers eager to lock him up and finally reaches the dubious sanctuary of a refugee camp.\\n\\nFacebook Twitter Pinterest The Syrian Super Mario video on YouTube\\n\\nSpeaking to the BBC, Mutfi – who himself fled Homs for Istanbul in 2011 – said: “Five months ago, my best friend drowned in the sea while travelling from Turkey to Greece: the engine on the boat exploded. That’s when I got the idea for the video. It needed to be a simple and clear idea that would work irrespective of language. I used Super Mario because it’s famous all over the world. It’s like music – a universal language.”\\n\\nMutfi, now working for a Turkish outfit, Online for Media, fled Homs after two of his brothers were killed. He has since made a number of animations parodying Assad’s speeches, having discovered that he can mimic his voice uncannily. Which may – along with his Syrian Super Mario video – have made him a marked man. But that doesn’t worry him: “We used to live in Syria without any dignity or freedom, but we don’t have anything now. It’s so disappointing to see what is happening with this refugee crisis. It’s life or death. We have to make our own futures.”\\n\\nMutfi’s Syrian Super Mario video is timely in one respect: last Friday, Nintendo released Super Mario Maker for Wii U, which lets gamers make their own Super Mario Bros levels – so anyone with a Wii U can use Mutfi’s efforts for inspiration. Could the “universal language” of gaming – which we know Cameron understands – be the medium that finally prompts the PM to take decisive action on Syria? Well, you never know.')]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardian_dic[list(guardian_dic.keys())[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('guardian.json', 'w') as fp:\n",
    "    json.dump(guardian_dic, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The FT - need to get corporate developer license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search_url = 'https://www.ft.com/search?q=mario+draghi'\n",
    "\n",
    "# r = requests.get(search_url)\n",
    "# soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "for li in soup.find_all('li', {\"class\": \"o-teaser-collection__item o-grid-row\"}):\n",
    "    article_links.extend(li.find_all('a'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = article_links[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://ft.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ParseArticles(base_url, article_links):\n",
    "    for link in article_links:\n",
    "        url = base_url + link\n",
    "        r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = base_url + link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()\n",
    "article.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subscribe to FT.com\\n\\nSubscribe to read: Italy’s president picks up pieces after political crisis\\n\\nAlready a subscriber? Sign in here'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EurActiv - has no authors on articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahdeen/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('http://www.euractiv.com/?s=mario+draghi').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block = soup.find_all(\"div\",{\"class\":\"row\"})\n",
    "article_links = []\n",
    "    \n",
    "for b in block.find_all(\"a\"):\n",
    "    if len(b['href'].split('/')) > 6 and b['href'] not in article_links:\n",
    "        article_links.append(b['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.euractiv.com/page/2/?s=mario+draghi\n",
      "http://www.euractiv.com/page/3/?s=mario+draghi\n",
      "http://www.euractiv.com/page/4/?s=mario+draghi\n",
      "http://www.euractiv.com/page/5/?s=mario+draghi\n",
      "http://www.euractiv.com/page/6/?s=mario+draghi\n",
      "http://www.euractiv.com/page/7/?s=mario+draghi\n",
      "http://www.euractiv.com/page/8/?s=mario+draghi\n",
      "You must `download()` an article before calling `parse()` on it!\n",
      "http://www.euractiv.com/page/9/?s=mario+draghi\n",
      "You must `download()` an article before calling `parse()` on it!\n"
     ]
    }
   ],
   "source": [
    "search_url = 'http://www.euractiv.com/?s=mario+draghi'\n",
    "# 'http://www.euractiv.com/page/2/?s=mario+draghi'\n",
    "# for all names, and 10 pages?\n",
    "\n",
    "def getLinksEurActiv(search_url):\n",
    "    r = requests.get(search_url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    block = soup.find_all(\"div\",{\"class\":\"row\"})[4]\n",
    "    article_links = []\n",
    "    \n",
    "    for b in block.find_all(\"a\"):\n",
    "        if len(b['href'].split('/')) > 6 and b['href'] not in article_links:\n",
    "            article_links.append(b['href'])\n",
    "    \n",
    "    return article_links\n",
    "\n",
    "\n",
    "def NewspaperParse(url, authors='authors'):\n",
    "    article = Article(url)   \n",
    "    \n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        r_datetime = article.publish_date\n",
    "        d = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "        t = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "        raw_text = article.text\n",
    "        if authors =='authors':\n",
    "            author = article.authors[0]\n",
    "        else:\n",
    "            author = 'none'\n",
    "        return d, t, raw_text, author\n",
    "    except:\n",
    "        return 'none', 'none', 'none', 'none'\n",
    "        \n",
    "\n",
    "article_links = []\n",
    "euractiv_dic = {}\n",
    "\n",
    "for i in range(2,10):\n",
    "    search_url = 'http://www.euractiv.com/page/{}/?s=mario+draghi'.format(i)\n",
    "    print(search_url)\n",
    "    links = getLinksEurActiv(search_url)\n",
    "    article_links.extend(links)\n",
    "    article_links = list(set(article_links))\n",
    "    \n",
    "    for link in article_links:\n",
    "        d, t, raw_text, author = NewspaperParse(link, 'none')\n",
    "        if d != 'none':\n",
    "            if d in euractiv_dic:\n",
    "                euractiv_dic[d].append([(author, t, raw_text)])\n",
    "            else:\n",
    "                euractiv_dic[d] = [(author, t, raw_text)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31656  100 31656    0     0  38823      0 --:--:-- --:--:-- --:--:-- 38794\n"
     ]
    }
   ],
   "source": [
    "'http://content.guardianapis.com/search?section=business&from-date=2001-01-01&page-size=161&q=vitor%20constancio&api-key=458f61b9-2ff0-4a3f-98a1-feff65660ca6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewspaperScraper():\n",
    "\n",
    "\tdef newspaper(url, authors='authors'):\n",
    "\t\tarticle = Article(url)   \n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tarticle.download()\n",
    "\t\t\tarticle.parse()\n",
    "\t\t\tr_datetime = article.publish_date\n",
    "\t\t\td = datetime.strftime(r_datetime, '%Y-%m-%d')\n",
    "\t\t\tt = datetime.strftime(r_datetime, '%H:%M:%S')\n",
    "\t\t\traw_text = article.text\n",
    "\t\t\tif authors =='authors':\n",
    "\t\t\t    author = article.authors[0]\n",
    "\t\t\telse:\n",
    "\t\t\t    author = 'none'\n",
    "\t\treturn d, t, raw_text, author\n",
    "\texcept:\n",
    "\treturn 'none', 'none', 'none', 'none'\n",
    "\n",
    "\n",
    "\tdef dictify(dic, d, t, author, raw_text):\n",
    "\t\tif d in dic:\n",
    "\t\t\tdic[d].extend([(author, t, raw_text)])\n",
    "\t\telse:\n",
    "\t\t\tdic[d] = [(author, t, raw_text)]\n",
    "\t\treturn dic\t\n",
    "\n",
    "\n",
    "class Guardian(NewspaperScraper):\n",
    "\t\n",
    "\tbase_url = 'http://www.theguardian.com' # same for every instance\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tself.dic = {} # creates a new empty dic for each Guardian class\n",
    "\n",
    "\tdef getlinks(self, search_url):\n",
    "\t\t#  uses the api unlike others\n",
    "\n",
    "\tdef parse(self, base_url=None, article_links):\n",
    "\t\t\n",
    "\t\tif article_links = None:\n",
    "\t\t\traise Exception.message('No links! Run getlinks() before parse()')\n",
    "\n",
    "\t\tfor link in article links:\n",
    "\t\t\td, t, author, raw_text = newspaper(link)\n",
    "\t\t\tself.dictify(self.dic, d, t, author, raw_text)\n",
    "\n",
    "\t\treturn self.dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://content.guardianapis.com/search?section=business&from-date=2001-01-01&page-size=161&q=vitor%20constancio&api-key=458f61b9-2ff0-4a3f-98a1-feff65660ca6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# json.loads(r.text)['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# import time\n",
    "\n",
    "# firefox = webdriver.Firefox()\n",
    "\n",
    "# gmail_url = \"https://mail.google.com\"\n",
    "\n",
    "# firefox.get(gmail_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
